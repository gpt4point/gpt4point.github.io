<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="GPT4Point">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GPT4Point</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>


<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
  
	/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
	.rainbow-text {
	  background: linear-gradient(to right, rgba(211, 242, 248, 1), rgba(246, 214, 214, 1));
	  -webkit-background-clip: text;
	  color: transparent;
	  display: inline-block;
	  font-weight: bold;
	}
  
</style>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img id="logo" width="5%" src="static/images/icon.png"> <span class="rainbow-text">GPT4Point</span>: A Unified Framework for Point-Language Understanding and Generation</h1>
          <div class="is-size-5 publication-authors"> 
            <span class="author-block"> <a href="https://github.com/Qi-Zhangyang">Zhangyang Qi </a><sup>1,&#9733</sup>, </span> 
            <span class="author-block"> <a href="https://github.com/Aleafy">Ye Fang</a><sup>2,5,&#9733</sup>, </span> 
            <span class="author-block"> <a href="https://github.com/SunzeY">Zeyi Sun</a><sup>3,5,&#9733</sup>, </span> 
            <span class="author-block"> <a href="https://xywu.me">Xiaoyang Wu</a><sup>1</sup>, </span> 
          </div> 
    
          <div class="is-size-5 publication-authors"> 
            <span class="author-block"> <a href="https://wutong16.github.io/">Tong Wu</a><sup>4</sup>, </span> 
            <span class="author-block"> <a href="https://myownskyw7.github.io/">Jiaqi Wang</a><sup>5,&#9993</sup>, </span> 
            <span class="author-block"> <a href="http://dahua.site/">Dahua Lin</a><sup>4,5</sup>, </span> 
            <span class="author-block"> <a href="https://hszhao.github.io/">Hengshuang Zhao</a><sup>1,&#9993 </sup> 
          </div>

          <div class="is-size-5 publication-authors"> 
            <span class="author-block" style="font-size: smaller;"> &#9733 Equal Contribution, </span> 
            <span class="author-block" style="font-size: smaller;"> &#9993 Correspending Authors </span> 
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong, </span>
            <span class="author-block"><sup>2</sup>Fudan University, </span
            <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University, </span> <br>
            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong, </span>
            <span class="author-block"><sup>5</sup>Shanghai Artificial Intelligence Lab,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block"> <a href="https://arxiv.org/abs/2312.02980"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>Paper (arXiv)</span> </a> </span>
              <!-- Demo Link. -->
              <span class="link-block"> <a href="https://www.youtube.com/"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fa fa-gamepad"></i> </span> <span>Demo (Hugging Face) (Coming Soon)</span> </a> </span>
              <!-- Code Link. -->
              <span class="link-block"> <a href="https://github.com/Pointcept/PointBLIP"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span>
          	</div>
       	  </div>
        </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
	  <!--
		<div style="text-align: center;">
				<img id="teaser" width="100%" src="static/images/teaser.png">     
			  </div><br>
	  -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
		  <style>
			/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
			.rainbow-text {
			  background: linear-gradient(to right, rgb(27, 201, 236), rgb(137, 23, 231));
			  -webkit-background-clip: text;
			  color: transparent;
			  display: inline-block;
			  font-weight: bold;
			}
		  </style>
          <p>
            Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for <b>unified 3D object understanding and generation within the MLLM framework</b>. <span class="rainbow-text">GPT4Point</span> as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A. 
            Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors. To support the expansive needs of 3D object-text pairs, we develop <b>Pyramid-XL, a point-language dataset annotation engine</b>. It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point. <b>A comprehensive benchmark</b> has been proposed to evaluate 3D point-language understanding capabilities. 
            In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

<section class="section"  style="background-color:#efeff081" id="Highlight">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">üî•Highlight</h2>
            <div class="content has-text-justified">
              <p style="font-size: 15px;">
                <ul>
				  <li><b>Unified Framework for Point-language Understanding and Generation.</b> 
            We present the unified framework for point-language understanding and generation GPT4Point, including the 3D MLLM for point-text tasks and <span style="color: rgb(137, 23, 231);">controlled 3D generation</span>.
          </li>
				  <li><b>Automated Point-language Dataset Annotation Engine Pyramid-XL.</b> 
            We introduce the automated point-language dataset annotation engine <span style="color: rgb(137, 23, 231);"> Pyramid-XL</span> based on Objaverse-XL, currently encompassing 1M pairs of varying levels of coarseness and can be extended cost-effectively.
          </li>
				  <li><b>Object-level Point Cloud Benchmark.</b> 
            Establishing a novel object-level point cloud benchmark with comprehensive evaluation metrics for 3D point cloud language tasks. 
            This benchmark thoroughly assesses models' understanding capabilities and facilitates the evaluation of generated 3D objects.
          </li>
				</ul>
              </p>
            </div>
          </div>
        </div>
      </div>
</section><br>

<section class="section" id="GPT4Point Usage">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> GPT4Point-Usage</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Task examples of GPT4Point.</b> It performs accurate 3D recognition, detailed captioning, precise Q&A, and high-quality controllable 3D generation. Additionally, GPT4Point excels in 3D anomalous object description, accurately assessing abnormal shapes like the multi-face object and the 3D generation failure case. 
                It is a crucial ability in the assessment of generated 3D objects. 
				<centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/fig1_teaser.png">     
                  </div><br>
                <centering>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="GPT4Point pipeline">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> GPT4Point Pipeline</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>The model architecture of GPT4Point for training.</b> In Stage1, we employ a Bert-based Point-Q-Former for point-text feature alignment through three point-text tasks. Then, in Stage2, an LLM is appended to train the model's text inference capabilities. 
                A Point Cloud Diffusion is attached separately to train controlled text-to-3D generation which keeps the geometry shape and colors.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/fig2_architechure.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Pyramid-XL: An automated point-text annotation engine">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Pyramid-XL: An automated point-text annotation engine</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Pyramid-XL: An automated point-text annotation engine.</b> Directly inputting images into VLMs yields unsatisfactory results.
                We propose a progressive annotation approach with 3 levels of granularity, leveraging results from the previous level for precise outcomes.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/fig3_dataset_engine.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section> 


    <section class="section" id="GPT4Point-Long Caption and Question & Answering">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> GPT4Point: Long Caption and Question & Answering</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Examples of text inference using the GPT4Point with ViT-g and OPT6.7B after Instruct Finetuning.</b> The table showcases
                its proficiency with point cloud input, excelling in tasks like detailed caption generation and point cloud-bas question answering. This
                underscores our model's profound grasp of point cloud geometry and color, translating them into meaningful semantics.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/fig4_qa_demo.png">     
                  </div>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/sup_fig10_qa_demo.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="GPT4Point: Detect Anomalous">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">GPT4Point: Detect Anomalous Objects(Generation Failure Cases)</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Anomalous Objects: Generation Failure Cases.</b> The upper and lower parts respectively depict the performance of 2D
                MLLM and GPT4Point in identifying abnormally generated objects with multi-body and multi-head structures. GPT4Point is effective in
                making accurate judgments, whereas 2D MLLM, due to the lack of information from single-view images, fails to identify most cases.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/sup_fig9_qa_special.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="GPT4Point: Point Diffusion Results">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">GPT4Point: Point Diffusion Results</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Point Diffusion Results: our controllable text-to-3D.</b> Given a low-quality point cloud prior, it can generate outcomes
                  superior to direct text-to-3D and image-to-3D methods and more closely align with the low-quality priors, demonstrating controllability.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/sup_fig5_point_diffusion_results.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="Pyramid-XL Level 3 Point-E Finetune Results">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Pyramid-XL Level 3 Point-E Finetune Results.</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                <b>Pyramid-XL Level 3 Point-E Finetune Results.</b> We found that the results of fine-tuning with dense captions from our
                Pyramid-XL significantly outperform those fine-tuned with Cap3D captions, demonstrating the greater accuracy of our generated captions.                
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/sup_fig11_dataset_pointe_ft_results.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
          @misc{qi2023gpt4point,
            title={GPT4Point: A Unified Framework for Point-Language Understanding and Generation}, 
            author={Zhangyang Qi and Ye Fang and Zeyi Sun and Xiaoyang Wu and Tong Wu and Jiaqi Wang and Dahua Lin and Hengshuang Zhao},
            year={2023},
            eprint={2312.02980},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
      }
      </code></pre>
      </div>
    </section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://jerryxu.net/GroupViT">GroupViT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
