<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Alpha-CLIP">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Alpha-CLIP</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>


<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
  
	/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
	.rainbow-text {
	  background: linear-gradient(to right, #3498db, #2ecc71);
	  -webkit-background-clip: text;
	  color: transparent;
	  display: inline-block;
	  font-weight: bold;
	}
  
</style>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img id="logo" width="5%" src="static/images/icon.png"> <span class="rainbow-text">Alpha-CLIP</span>: A CLIP Model Focusing on Wherever You Want</h1>
          <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://github.com/SunzeY">Zeyi Sun</a><sup>1,3</sup>,</span> <span class="author-block"> <a href="https://github.com/Aleafy">Ye Fang</a><sup>2,3</sup>,</span> <span class="author-block"> <a href="https://wutong16.github.io/">Tong Wu</a><sup>3,4</sup>, </span> <span class="author-block"> <a href="https://panzhang0212.github.io/">Pan Zhang</a><sup>3</sup>, </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://yuhangzang.github.io/">Yuhang Zang</a><sup>3</sup>, </span> <span class="author-block"> <a href="https://aimerykong.github.io/">Shu Kong</a><sup>5,6</sup>, </span> <span class="author-block"> <a href="http://yjxiong.me/">Yuanjun Xiong</a><sup>7</sup>, </span> <span class="author-block"> <a href="http://dahua.site/">Dahua Lin</a><sup>3,4</sup>, </span> <span class="author-block"> <a href="https://myownskyw7.github.io/">Jiaqi Wang</a><sup>3</sup> </span> </div>
          <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span> <span class="author-block"><sup>2</sup>Fudan University,</span> <span class="author-block"><sup>3</sup>Shanghai Artificial Intelligence Lab,</span><br>
              <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong,</span> <span class="author-block"><sup>5</sup>Texas A&M University,</span> <span class="author-block"><sup>6</sup>University of Macau,</span> <span class="author-block"><sup>7</sup>Amazon AI</span> </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block"> <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span>
              <!-- Video Link. -->
              <span class="link-block"> <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-youtube"></i> </span> <span>Video</span> </a> </span>
              <!-- Code Link. -->
              <span class="link-block"> <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span>
              <!-- Dataset Link. -->
              <span class="link-block"> <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Dataset</span></a></span></div>
          	 </div>
       	   </div>
        </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
	  <!--
		<div style="text-align: center;">
				<img id="teaser" width="100%" src="static/images/teaser.png">     
			  </div><br>
	  -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
		  <style>
			/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
			.rainbow-text {
			  background: linear-gradient(to right, #3498db, #2ecc71);
			  -webkit-background-clip: text;
			  color: transparent;
			  display: inline-block;
			  font-weight: bold;
			}
		  </style>
          <p>
            Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models.
          </p>
          <p>
            To fulfill the requirements, we introduce <span class="rainbow-text">Alpha-CLIP</span>, an enhanced version of CLIP with <b>an auxiliary alpha channel</b> to suggest attentive regions and fine-tuned with constructed <b> millions of RGBA region-text</b> pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to <b>open-world recognition</b>, <b>multimodal large language models</b>, and <b>conditional 2D / 3D generation</b>. It has a strong potential to serve as a versatile tool for image-related tasks. All the code, models, and training data will be publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

<section class="section"  style="background-color:#efeff081" id="Highlight">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">üî•Highlight</h2>
            <div class="content has-text-justified">
              <p style="font-size: 15px;">
                <ul>
				  <li><b>Alpha-CLIP in Image Recognition.</b> 
Focusing on a specified ground-truth region leads to a 4.1% increase in top-1 accuracy for the zero-shot ImageNet classification task. This <span style="color: #997300;">enhanced region-based recognition </span>is valuable for tasks like Referring Expression Comprehension(REC) and serves as data engine for Open Vocabulary Detection(OVD).</li>
				  <li><b>Serving as vision backbone in MLLM.</b> In conjunction with a large language model, Alpha-CLIP becomes capable of facilitating <span style="color: #997300;"> region level captioning and VQA</span> within a MLLM framework(e.g. BLIP-2, LLaVA). This integration significantly mitigates the occurrences of hallucinations and diminishes model bias, while also empowers region-focused new tasks. </li>
				  <li><b>Alpha-CLIP in 2D generation.</b> When integrated with a diffusion model, Alpha-CLIP enhances the controllability of BLIP-Diffusion in image variation tasks. In addition, it enables the extraction of subjects from <span style="color: #997300;"> complex images for subject-driven generation</span>, surmounting an obstacle encountered when deploying BLIP-Diffusion with the original CLIP, which only supports single subjects in simplistic images. </li>
				  <li><b>Alpha-CLIP in 3D generation.</b> In addition to the capabilities in 2D generation, Alpha-CLIP exhibits proficiency in 3D generation as well. It can be effectively deployed in conjunction with a diffusion model, such as Point-E, to <span style="color: #997300;"> enhance the quality of 3D object generation</span>. Additionally, it can be utilized with NeRF, exemplified by PureCLIPNeRF, to optimize the creation of superior 3D objects. </li>
				</ul>
              </p>
            </div>
          </div>
        </div>
      </div>
</section><br>

<section class="section" id="Alpha-CLIP usage">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Usage</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                Alpha-CLIP can enhance CLIP across a wide array of downstream tasks, applying a <b>plug-and-play</b> methodology that permeates diverse domains, spanning from <b>perception to generation</b> in <b>2D and 3D applications</b>. We show downstream tasks of Alpha-CLIP and their advantages over the original CLIP in the following figure and table.
				<centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/teaser.png">     
                  </div><br>
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/usage_table1.png">     
                  </div> 
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="Alpha-CLIP pipeline">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Pipeline</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                The pipeline of our <b>data generation method</b> and <b>model architecture</b>. (a) Our method generates millions of RGBA region-text pairs. (b) Alpha-CLIP modifies the CLIP image encoder to take an additional alpha channel along with RGB. We first generate millions of RGBA region-text data from grounding and classification datasets. Using our generated data, we then train our Alpha-CLIP with additional Alpha-channel inputs.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/pipeline.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in MLLM</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                We replace CLIP used in BLIP-2 and LLaVA-1.5 with our Alpha-CLIP to make MLLM directly focus on <b>user-defined region</b> in vision-language tasks such as <b>region level captioning</b> and <b>VQA</b>. All cases shown here are made simply by replacing the original CLIP of BLIP2 or LLaVA-1.5 with a <b>plug-in</b> Alpha-CLIP without further tuning.
                <centering>
                  <div style="text-align: center;">
                    <img id="mllm" width="100%" src="static/images/mllm_more1.png">     
                  </div>
              </p>
            </div> <br>
			<!--      results-->
			<p>Besides qualitative results, we also provide quantitative region captioning results of Alpha-CLIP with LLaVA-1.5 on Visual Genome and RefCOCOg.</p>
			<div style="text-align: center;">
				<table border="1" cellspacing="0" style="width: 92%; border-collapse: collapse; text-align: center; margin: 0 auto;">
					<caption><strong>Performance of Alpha-CLIP in Region-Level Captioning</strong>: We report METEOR and CIDEr metrics on Visual Genome and refCOCOg Datasets, demonstrating competitive outcomes.</caption>
					<thead>
						<tr>
							<th rowspan="2">Model</th>
							<th colspan="2">refCOCOg</th>
							<th colspan="2">Visual Genome</th>
						</tr>
						<tr>
							<th>METEOR</th>
							<th>CIDEr</th>
							<th>METEOR</th>
							<th>CIDEr</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>GRIT</td>
							<td>15.2</td>
							<td>71.6</td>
							<td>17.1</td>
							<td>142</td>
						</tr>
						<tr>
							<td>Kosmos-2</td>
							<td>14.1</td>
							<td>62.3</td>
							<td>-</td>
							<td>-</td>
						</tr>
						<tr>
							<td>GPT4RoI</td>
							<td>-</td>
							<td>-</td>
							<td>17.4</td>
							<td>145.2</td>
						</tr>
						<tr>
							<td>GLaMM</td>
							<td>16.2</td>
							<td>105.0</td>
							<td>18.6</td>
							<td>157.8</td>
						</tr>
						<tr style="background-color: #e6e6ff;">
							<td>Alpha-CLIP+LLaVA</td>
							<th>16.7</th>
							<th>109.2</th>
							<th>18.9</th>
							<th>160.3</th>
						</tr>
					</tbody>
				</table>
			</div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in Image Variation</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                Alpha-CLIP can be used in most image variation models that use CLIP image encoder. For example, BLIP-Diffusion bridges CLIP and stable-diffusion with Q-former to generate and edit 2D images controlled by text. By introducing Alpha-CLIP, we can add an additional set of vision prompts to allow the model to focus on <b>specified regions for 2D generation</b>, enabling <b>subject-driven generation in complex images</b>. The first row per three is the original BLIP-Diffusion generated
images. Other rows represent the outcomes of Alpha-CLIP with highlighted regions marked in red.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/append_more_diff.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="Alpha-CLIP in 3D">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in 3D Object Generation</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
			<div class="content has-text-justified">
			  <p><h2 class="title is-5">Alpha-CLIP in Point-E</h2> We demonstrate that Alpha-CLIP is helpful in two cases: 
			  <ol>
				<li>When Point-E generates the point cloud with some parts missing, users can highlight the missing part in the condition image to remind the diffusion model to pay more attention to that part and <b>fix this missing parts problem</b>.</li>
				<li>Users can <b>highlight the part that needs to be emphasized</b> on the 2D image.</li>
			  </ol>
				<div style="text-align: center;">
				<img src="static/gifs/pointe.gif" alt="First GIF" width="800" height="800">
				</div>
              <p><br><h2 class="title is-5">Alpha-CLIP in PureCLIPNeRF</h2> We replace CLIP model with Alpha-CLIP in PureCLIPNeRF to generate 3D objects, and tests are conducted with and without background augmentation. As shown in the following figure, with Alpha-CLIP, PureCLIPNeRF generates objects that <b>closely align with the provided textual prompts</b>(especially bolded text) in terms of shape and color. Furthermore, there is an enhancement in the <b>overall coherence of the generated objects</b>, coupled with notable <b>aesthetic qualities</b>. </p> </div>
            </b></font>
			<div style="text-align: center;">
			<img src="static/gifs/fountain0.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/fountain2.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/fountain3.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/fountain1.gif" alt="First GIF" width="250" height="250">
			<p>A <b>beautifully</b> carved <b>square</b> fountain with an <b>ornate statue</b> standing in the <b>center</b>.</p>
			<div style="text-align: center;">
			<img src="static/gifs/candle2.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/candle0.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/candle3.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/candle1.gif" alt="First GIF" width="250" height="250">
			<p>A <b>silver candlestick</b> with <b>several</b> burning candles.</p>
			<div style="text-align: center;">
			<img src="static/gifs/hat3.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/hat1.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/hat2.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/hat0.gif" alt="First GIF" width="250" height="250">
			<p>A unique <b>officer hat</b>, black with <b>gold trim</b>, adds a sense of authority.</p>
			<div style="text-align: center;">
			<img src="static/gifs/tokyo2.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/tokyo0.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/tokyo3.gif" alt="First GIF" width="250" height="250">
			<img src="static/gifs/tokyo1.gif" alt="First GIF" width="250" height="250">
			<p><b>Tokyo city</b>; trending on artstation.</p>
			</div>
			<centering>
			  <div style="text-align: center;">
				<img id="teaser" width="80%" src="static/images/pureclip.png">     
			  </div>
				<!-- Video
               <video id="pureclip" autoplay muted loop playsinline height="60%">
				<source src="./static/videos/pureclip2.mp4"
						type="video/mp4">
			  </video>
			  -->
				<!--Image
				<centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/3d_append_pureclip.png">     
                  </div>
				 -->
              
            
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in Image Recognition</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
			  ËøôÈáåÊç¢ÊàêË°®Ê†ºÔºåorÂèØ‰ª•Âä†‰∏Ä‰∏™clsÊµãËØïpipelineÔºõ ‰ª•Âèärec‰ªªÂä° 
			  <br>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/pipeline.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="Alpha-CLIP Attn">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Attention Map Visualization</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
			  	We check the attention map of [CLS] token in the last transformer block in the vision encoder.
Each first line per four is from original CLIP and the other three lines are from Alpha-CLIP with user-defined focus regions marked
in red. This visualization verifies that Alpha-CLIP <b>pays more attention to the area to focus on</b> and more importantly, with <b>no damage to the 2D location information</b> preserved in the feature location of the original CLIP.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/attn.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>



<section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
          @misc{sun2023alphaclip,
            title={Alpha-CLIP: A CLIP Model Focusing on Wherever You Want}, 
            author={Zeyi Sun and Ye Fang and Tong Wu and Pan Zhang and Yuhang Zang and Shu Kong and Yuanjun Xiong and Dahua Lin and Jiaqi Wang},
            year={2023},
            eprint={?},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
          }
      </code></pre>
      </div>
    </section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2210.04150.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/facebookresearch/ov-seg" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://jerryxu.net/GroupViT">GroupViT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
